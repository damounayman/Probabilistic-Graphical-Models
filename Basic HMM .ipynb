{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "class HMM(object): \n",
    "    # base class for different HMM models\n",
    "    def __init__(self, T, O, pi):\n",
    "        # model is (T, O, pi) where T = Transition probs(hidden_states*hidden_states), \n",
    "        # O = Emission Probs(hidden_states*states), pi = initial distribution(hidden_states)               \n",
    "        if T is None:\n",
    "            print (\"Error: You should provide the transition matrix\")\n",
    "            sys.exit() # Read in parameters from the file of model_name\n",
    "        if O is None:\n",
    "            print (\"Error: You should provide the emission matrix\")\n",
    "            sys.exit() # Read in parameters from the file of model_name\n",
    "        if pi is None:\n",
    "            print (\"Error: You should provide the initial probability\")\n",
    "            sys.exit() # Read in parameters from the file of model_name           \n",
    "        self.pi=pi\n",
    "        self.T=T\n",
    "        self.O=O\n",
    "        self.M=T.shape[1]  # M:number of hidden states of the model\n",
    "        self.N=O.shape[1]  # N:number of states of the model\n",
    "\n",
    "        \n",
    "    def backward(self, obs):\n",
    "        # This function is for backward algorithm, suppose that T, O, pi \n",
    "        #are given, and it calculates a bwk matrix (obs*states).\n",
    "        # The backward algorithm can be used to calculate the likelihood \n",
    "        #of the probability P(Y_{k+1}, ... , Y_n|t_k=C)\n",
    "        #=sum_q P(Y_{k+2}, ... , Y_n|t_{k+1}=q)P(q|C)P(x_{k+1}|q)\n",
    "        #The backward probability b is the probability of seeing the observations from \n",
    "        #time t + 1 to the end, given that we are in state i at time t\n",
    "        self.bwk = np.zeros(shape=(len(obs),self.M))\n",
    "        # Initalize bwk to be empty matrix T*M\n",
    "        # Initialize base cases (t == T)\n",
    "        self.bwk[len(obs)-1,:]=np.ones(self.M)      \n",
    "        for t in reversed(range(len(obs)-1)):\n",
    "            self.bwk[t,:] = np.sum(self.bwk[t+1,:] * self.T[:,:] * self.O[:,obs[t+1]],axis=1)        \n",
    "                #beta_k(C)=\\sum_q beta_{k+1}(q)P(q|C)P(w_{k+1}|q)\n",
    "        prob = np.sum(self.pi[0,:]* self.O[:,obs[0]] * self.bwk[0,:]) \n",
    "        return prob \n",
    "        #This prob is the likelihood of the input obs   \n",
    " \n",
    "\n",
    "    def forward(self, obs):\n",
    "        # This function is for forward algorithm, suppose that A, B, pi are given, \n",
    "        #and it calculates a fwd matrix (obs*states).\n",
    "        # The forward algorithm can be used to calculate the likelihood of the model\n",
    "        #P(Y1, ... , Yn)=sum_t(\\prod_i P(Y[i]|t[i])P(t[i]|t[i-1])\n",
    "        self.fwd = np.zeros(shape=(len(obs),self.M)) \n",
    "        #Initalize fwk to be empty matrix, and finally fwd is T*M\n",
    "        # Initialize base cases (t == 0)\n",
    "        self.fwd[0,:]=self.pi[0,:] * self.O[:,obs[0]] \n",
    "            #alpha_1(q)=p(w1,t1=q)=P(t1=q|t0)*p(w1|t1=q)\n",
    "        # Run Forward algorithm for t > 0            \n",
    "        for t in range(1, len(obs)):\n",
    "            self.fwd[t,:] = np.sum(self.fwd[t-1,:] * self.T[:,:] * self.O[:,obs[t]],axis=1) \n",
    "                #alpha_k(q)=\\sum_q1 alpha_{k-1}(q1)P(t_k=q|t_{k-1}=q1)P(w_k|t+k=q)\n",
    "        prob = np.sum(self.fwd[len(obs) - 1,:]) \n",
    "        # The likelihood of input equals to the summation of fwd[N][t]\n",
    "        return prob\n",
    "\n",
    "    \n",
    "    def viterbi(self, obs):\n",
    "    #the task of determining which sequence of variables is the underlying source \n",
    "    #of some sequence of observations is called the decoding task\n",
    "    #Decoding: Given as input an HMM = (A, B, pi) and a sequence of observations \n",
    "    #O = Y_1, ... Y_N, find the most probable sequence of states Q = X_1, ... X_T.\n",
    "    # Goal: find the best path!\n",
    "    # argmax_t P(Y1, ... Y_N, X_1, ..., X_T|A, B, pi)\n",
    "        vit = np.zeros(shape=(len(obs),self.M))\n",
    "        #[[0 for x in range(self.M)] for y in range(len(obs))] \n",
    "        # matrix\n",
    "        path=np.zeros(shape=(len(obs),self.M))\n",
    "        path[0,:]=range(self.M)\n",
    "        # path\n",
    "        # Initialize base cases (t == 0)\n",
    "        vit[0,:] = self.pi[0,:] * self.O[:,obs[0]]    \n",
    "        # Run Viterbi for t > 0      \n",
    "        for t in range(1, len(obs)):\n",
    "            vit[t,:]=np.max(vit[t-1,:] * self.T[:,:] * self.O[:,obs[t]],axis=1)\n",
    "            path[t,:]=np.argmax(vit[t-1,:] * self.T[:,:] * self.O[:,obs[t]],axis=1)\n",
    "        prob=np.max(vit[len(obs)-1,:])\n",
    "        ind=np.argmax(vit[len(obs)-1,:])\n",
    "        state=path[:,ind]\n",
    "        #(prob, state) = max((vit[n][y], y) for y in range(self.M))\n",
    "        return (prob, state)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward_backward(self, obs): \n",
    "        #Output matrix gamma: gamma[t][y]=P(q_t=j|Y_1, ..., Y_N,A,B,pi)\n",
    "        #and tensor zi: zi[t][i][j]=P(q_t=i,q_{t+1}=j|Y_1, ..., Y_N,A,B,pi)\n",
    "        gamma = np.zeros(shape=(len(obs),self.M))\n",
    "        # this is needed to keep track of finding a state i at a time t for all i and all t\n",
    "        zi= np.zeros(shape=(len(obs),self.M,self.M))\n",
    "        #zi = [[[0 for x in range(self.M)] for y in range(self.M)] for z in range(len(obs))]  \n",
    "        # this is needed to keep track of finding a state i at a time t and j at a time (t+1) \n",
    "        #for all i and all j and all t\n",
    "        # get alpha and beta tables computes\n",
    "        p_obs = self.forward(obs)\n",
    "        self.backward(obs)\n",
    "        # compute gamma values\n",
    "        for t in range(len(obs)):\n",
    "            gamma[t,:] = (self.fwd[t,:] * self.bwk[t,:]) / p_obs\n",
    "            if t == 0:\n",
    "                self.pi[0,:] = gamma[t,:]\n",
    "                #gamma[t][y]=P(q_t=j|Y_1, ..., Y_N,A,B,pi)\n",
    "                #=P(q_t=j,Y_1, ..., Y_N|A,B,pi)/P(Y_1, ..., Y_N|A,B,pi)\n",
    "                #=alpha_t(j)beta_t(j)/P(Y_1, ..., Y_N|A,B,pi)\n",
    "                #compute zi values up to T - 1\n",
    "            if t == len(obs) - 1:\n",
    "                continue\n",
    "            for y1 in range(self.M):\n",
    "                zi[t,:,:] = self.fwd[t,:] * self.T[:,:] * self.O[:,obs[t + 1]] * self.bwk[t + 1,:] / p_obs\n",
    "        #zi[t][i][j]=P(q_t=i,q_{t+1}=j|Y_1, ..., Y_N,A,B,pi)       \n",
    "        #=P(q_t=i,q_{t+1}=j,Y_1, ..., Y_N|A,B,pi)/P(Y_1, ..., Y_N|A,B,pi)\n",
    "        #=alpha_t(i)a_{ij}b_j(O_{t+1})beta_{t+1}(j)/apha_t(X_T)\n",
    "        return (gamma,zi)\n",
    "    \n",
    "    \n",
    "    def baum_welch(self,obs):\n",
    "        #returns model given the initial model and observations  \n",
    "        #The Baum-Welch algorithm iteratively estimate the counts.\n",
    "        #We will start with an estimate for the transition and observation probabilities and \n",
    "        #then use these estimated probabilities to derive better and better probabilities. \n",
    "        #We get our estimated probabilities by computing the forward probability for \n",
    "        #an observation and then dividing that probability mass among all the different \n",
    "        #paths that contributed to this forward probability.\n",
    "        gamma = np.zeros(shape=(len(obs),self.M))\n",
    "        zi =  np.zeros(shape=(len(obs),self.M,self.M))\n",
    "        # now that we have gamma and zi let us re-estimate\n",
    "        (gamma,zi)=self.forward_backward(obs)\n",
    "        \n",
    "        #Update T\n",
    "        #T_{ij)=expected number of transitions from state i to state j/expected number \n",
    "        #of transitions from state i\n",
    "        a=np.sum(zi,axis=(0,2))\n",
    "        self.T=np.sum(zi,axis=0)/np.array([a,]*self.M).transpose()\n",
    "        \n",
    "        for y in range(self.M):\n",
    "            for k in range(self.N): \n",
    "                # for all symbols vk\n",
    "                val = 0.0\n",
    "                for t in range(len(obs)):\n",
    "                    if obs[t] == k :\n",
    "                        val += gamma[t][y]\n",
    "                val /= np.sum(gamma[:,y])\n",
    "                self.O[y][k] = val\n",
    "                #O_j(v_k)=expected number of times in state j and observing symbol vk/expected \n",
    "                #number of times in state j\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=3\n",
    "N=3\n",
    "T = np.array([[0, 0.2, 0.8],\n",
    "[0, 0.6, 0.4],\n",
    "[0, 0.5, 0.5]])\n",
    "# Get transition probability\n",
    "\n",
    "O = np.array([[0.33, 0.33, 0.33],\n",
    "[0.2, 0.4, 0.4],\n",
    "[0.5, 0.4, 0.1]])\n",
    "# Get emission probability\n",
    "pi_raw = np.random.random((1, M)) \n",
    "row_sums_pi = pi_raw.sum(axis=1)\n",
    "pi = pi_raw / row_sums_pi[:, np.newaxis]\n",
    "\n",
    "# Get initial probability\n",
    "hmm=HMM(T,O,pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M= 3 N= 3 Observations =  [0, 0, 1, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "#hmm=HMM(M,N)\n",
    "observations = []\n",
    "observations = [0, 0, 1, 2, 2, 1]\n",
    "print(\"M=\", M, \"N=\", N, \"Observations = \", observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bwk Prob =  0.0014549839338601146\n"
     ]
    }
   ],
   "source": [
    "p1=hmm.backward(observations)\n",
    "print(\" Bwk Prob = \", p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fwd Prob =  0.0018602694040986056\n"
     ]
    }
   ],
   "source": [
    "p2=hmm.forward(observations)\n",
    "print(\" Fwd Prob = \", p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Probability =  0.0001489589889762435  Hidden State Sequence =  [0. 2. 2. 2. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "prob, hidden_states = hmm.viterbi(observations)\n",
    "print(\"Max Probability = \", prob, \" Hidden State Sequence = \", hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
